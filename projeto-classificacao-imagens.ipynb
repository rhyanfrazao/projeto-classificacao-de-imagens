{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e22dd1a-1285-4655-90dc-5f4a083ae145",
   "metadata": {},
   "source": [
    "# <a>Classificação de Imagens - PyTorch e Transfer Learning</a>\n",
    "\n",
    "Projeto realizado durante a Jornada Cientista de Dados da equipe <i>Let's Data</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb445d1-c78d-429c-8649-74f048c4bab6",
   "metadata": {},
   "source": [
    "# <a>Objeto de Estudo</a>\n",
    "\n",
    "Iremos modelar um Produto Mínimo Viável que possa ser testado nas filiais de uma empresa do ramo de supermercados. O problema a ser solucionado é classificar produtos na hora de colocar no mostruário. Vamos começar com uma classificação simples de batatas, cenouras, tomates e limões.\n",
    "\n",
    "Para isso iremos usar modelos pré-treinados para nos ajudar com a classificação de imagens através dos padrões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f8ab2-7ec8-49cd-97ca-b6f531adb03a",
   "metadata": {},
   "source": [
    "<b>Criação e Separação de Bases:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f4f496-fc7d-4e8b-abbe-d117ce6063ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tbb in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (2021.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6d5f8f-bfea-4273-a16c-c5399f98d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfb279b-f3b5-4417-975f-39633277cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\rhyan\\appdata\\roaming\\python\\python310\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bae816-e3d8-4d85-a537-d34dd3c44c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\rhyan\\appdata\\roaming\\python\\python310\\site-packages (0.18.1)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\rhyan\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (2022.11.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (3.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from torch==2.3.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rhyan\\anaconda3\\lib\\site-packages (from sympy->torch==2.3.1->torchvision) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3f20cd-9050-4fbe-bf4f-ec1a82d93ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchvision\n",
      "Version: 0.18.1\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: c:\\users\\rhyan\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: numpy, pillow, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8089f6ed-f80e-45c2-a0de-1256dd56f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias:\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e68e06b-64c0-46f4-bee4-6729e034edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "677c96fb-2c8e-4fd7-a8ab-a99a1c6e68c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batata', 'cenoura', 'limao', 'tomate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos separar as imagens em bases de treino, validação e teste\n",
    "# Mas antes vamos deixar salva uma pasta raw com as originais\n",
    "\n",
    "diretorio_base_imagens = 'C:\\\\data_science\\\\projeto-classificacao-imagens\\\\data\\\\raw'\n",
    "pastas_com_nome_de_vegetais = os.listdir('C:\\\\data_science\\\\projeto-classificacao-imagens\\\\data\\\\raw')\n",
    "pastas_com_nome_de_vegetais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e78a26a-3503-469e-a693-c99c016c0e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batata', 'cenoura', 'limao', 'tomate']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pastas_com_nome_de_vegetais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c457581a-dcce-445d-bd38-63855b5241b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batata': 146, 'cenoura': 181, 'limao': 111, 'tomate': 107}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos criar uma separação estratificada:\n",
    "# Queremos 80% das imagens na base de treino, 10% na validação e 10% no teste\n",
    "\n",
    "quantidade_por_label = {pasta: len(os.listdir(os.path.join(diretorio_base_imagens, pasta))) for pasta in pastas_com_nome_de_vegetais}\n",
    "quantidade_por_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65d0842-cf44-4c65-97b9-26f47a3b1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as pastas de treino, validação e testes\n",
    "\n",
    "diretorio_imagens_processadas = 'C:\\\\data_science\\\\projeto-classificacao-imagens\\\\data\\\\processed'\n",
    "\n",
    "dir_treino = os.path.join(diretorio_imagens_processadas, 'treino')\n",
    "dir_validacao = os.path.join(diretorio_imagens_processadas, 'validacao')\n",
    "dir_teste = os.path.join(diretorio_imagens_processadas, 'teste')\n",
    "\n",
    "if not os.path.exists(dir_treino):\n",
    "    os.makedirs(dir_treino)\n",
    "\n",
    "if not os.path.exists(dir_validacao):    \n",
    "    os.makedirs(dir_validacao)\n",
    "\n",
    "if not os.path.exists(dir_teste):\n",
    "    os.makedirs(dir_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fad67e1-776e-421d-8185-0bce0ee11eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batata - treino: 116 - valid: 15 - teste: 15 - total: 146\n",
      "cenoura - treino: 144 - valid: 18 - teste: 19 - total: 181\n",
      "limao - treino: 88 - valid: 11 - teste: 12 - total: 111\n",
      "tomate - treino: 85 - valid: 11 - teste: 11 - total: 107\n"
     ]
    }
   ],
   "source": [
    "#Separando as imagens nas bases:\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Criando uma pasta para cada classe (batata, cenoura, limao, tomate)\n",
    "# dentro de treino, validação e teste\n",
    "\n",
    "for classe in pastas_com_nome_de_vegetais:\n",
    "    # os.path.join cria paths com os separadores corretos pra cada sistema operacional\n",
    "    # barra normal, barra invertida, isso muda do Windows pro Linux/Mac\n",
    "    dir_classe_treino = os.path.join(dir_treino, classe)\n",
    "    dir_classe_validacao = os.path.join(dir_validacao, classe)\n",
    "    dir_classe_teste = os.path.join(dir_teste, classe)\n",
    "    \n",
    "    # Efetivamente criando as pastas de treino, validação e teste\n",
    "    # Testa primeiro se as pastas já não existem\n",
    "    if not os.path.exists(dir_classe_treino):\n",
    "        os.makedirs(dir_classe_treino)\n",
    "\n",
    "    if not os.path.exists(dir_classe_validacao):\n",
    "        os.makedirs(dir_classe_validacao)\n",
    "    \n",
    "    if not os.path.exists(dir_classe_teste):\n",
    "        os.makedirs(dir_classe_teste)\n",
    "    \n",
    "    # caminho completo para a pasta com imagens originais\n",
    "    pasta_classe = os.path.join(diretorio_base_imagens, classe)\n",
    "    \n",
    "    # listando todos os arquivos de imagem para essa classe\n",
    "    arquivos_classe = os.listdir(pasta_classe)\n",
    "    \n",
    "    # separando 80% para treino e 20% para validação+teste\n",
    "    treino, valid_teste = train_test_split(arquivos_classe, \n",
    "                                           shuffle=True, \n",
    "                                           test_size=0.2, \n",
    "                                           random_state=42)\n",
    "    \n",
    "    # separando os 20% da validação+teste em 10% para validação e 10% para teste\n",
    "    validacao, teste = train_test_split(valid_teste, shuffle=True, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Não precisamos mais dessa lista temporária\n",
    "    del valid_teste\n",
    "    \n",
    "    print(f'{classe} - treino: {len(treino)} - valid: {len(validacao)} - teste: {len(teste)} - total: {len(arquivos_classe)}')\n",
    "    \n",
    "    # Copiando os arquivos efetivamente para as pastas de treino, validação e teste\n",
    "    for imagem_treino in treino:\n",
    "        caminho_origem = os.path.join(diretorio_base_imagens, classe, imagem_treino)\n",
    "        caminho_destino = os.path.join(dir_classe_treino, imagem_treino)\n",
    "\n",
    "        shutil.copy(caminho_origem, caminho_destino)\n",
    "\n",
    "    for imagem_validacao in validacao:\n",
    "        caminho_origem = os.path.join(diretorio_base_imagens, classe, imagem_validacao)\n",
    "        caminho_destino = os.path.join(dir_classe_validacao, imagem_validacao)\n",
    "\n",
    "        shutil.copy(caminho_origem, caminho_destino)\n",
    "\n",
    "    for imagem_teste in teste:\n",
    "        caminho_origem = os.path.join(diretorio_base_imagens, classe, imagem_teste)\n",
    "        caminho_destino = os.path.join(dir_classe_teste, imagem_teste)\n",
    "\n",
    "        shutil.copy(caminho_origem, caminho_destino)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39316ae6-44a8-407a-bb5c-2375b24ceac9",
   "metadata": {},
   "source": [
    "<b>Pré Processamento:</b>\n",
    "\n",
    "Vamos efetivamente processar as imagens para utilizar no modelo do Pytorch. Vamos criar as transformações para redimensionar as imagens e transformá-las em tensores Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5c1430-2f6c-4651-a715-f54f23c83183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setando o tamanho da imagem\n",
    "image_size = 100\n",
    "\n",
    "# Transformando as imagens: para modelos mais robustos tem que caprichar no data augmentation!\n",
    "# Nesse caso não fizemos nada além do redimensionamento da imagem, mas é sempre bom fazer rotações,\n",
    "# espelhamentos, crop randomicos pra garantir \n",
    "transformacoes_de_imagens = { \n",
    "    'treino': transforms.Compose([\n",
    "        transforms.Resize(size=[image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'validacao': transforms.Compose([\n",
    "        transforms.Resize(size=[image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'teste': transforms.Compose([\n",
    "        transforms.Resize(size=[image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cccd614b-2777-4a41-8a3a-1ded2991a561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\data_science\\\\projeto-classificacao-imagens\\\\data\\\\processed\\\\treino',\n",
       " 'C:\\\\data_science\\\\projeto-classificacao-imagens\\\\data\\\\processed\\\\validacao',\n",
       " 'C:\\\\data_science\\\\projeto-classificacao-imagens\\\\data\\\\processed\\\\teste')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar as imagens\n",
    "# Determinar as pastas de treino, validação e teste\n",
    "\n",
    "pasta_treino = dir_treino\n",
    "pasta_validacao = dir_validacao\n",
    "pasta_teste = dir_teste\n",
    "\n",
    "pasta_treino, pasta_validacao, pasta_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f90827-6d6f-40d3-920f-eed15e062699",
   "metadata": {},
   "source": [
    "<b>Preparação para o treinamento:</b>\n",
    "\n",
    "Vamos definir informações importantes para o treinamento do modelo. Tamanho do batch, número de classes, datasets, data loaders, otimizadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "226df0a4-9b40-43f6-ad6d-5bbb388d0ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamanho do batch de treinamento\n",
    "tamanho_do_batch = 8\n",
    "\n",
    "# Determinando o número de classes (verduras/frutas)\n",
    "numero_de_classes = len(os.listdir(pasta_treino))\n",
    "\n",
    "numero_de_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035de28c-5d47-4e8a-92e3-3b3be1b36b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar as imagens usando o datasets do torchvision\n",
    "data = {\n",
    "    'treino': datasets.ImageFolder(root=pasta_treino, transform=transformacoes_de_imagens['treino']),\n",
    "    'validacao': datasets.ImageFolder(root=pasta_validacao, transform=transformacoes_de_imagens['validacao'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e647c6-9e94-4cd5-a63e-b9162f8fc6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'treino': Dataset ImageFolder\n",
       "     Number of datapoints: 433\n",
       "     Root location: C:\\data_science\\projeto-classificacao-imagens\\data\\processed\\treino\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=[100, 100], interpolation=bilinear, max_size=None, antialias=True)\n",
       "                ToTensor()\n",
       "            ),\n",
       " 'validacao': Dataset ImageFolder\n",
       "     Number of datapoints: 55\n",
       "     Root location: C:\\data_science\\projeto-classificacao-imagens\\data\\processed\\validacao\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=[100, 100], interpolation=bilinear, max_size=None, antialias=True)\n",
       "                ToTensor()\n",
       "            )}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d422b6d9-d053-44d6-9746-0910ba1fc096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'batata', 1: 'cenoura', 2: 'limao', 3: 'tomate'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapear os índices com os nomes das classes\n",
    "indice_para_classe = {indice: classe for classe, indice in data['treino'].class_to_idx.items()}\n",
    "\n",
    "indice_para_classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47a74a1b-fcfb-42fb-a60d-520af224d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 55)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantidade de imagens para serem utilizados para calcular erro médio e acurácia\n",
    "num_imagens_treino = len(data['treino'])\n",
    "num_imagens_validacao = len(data['validacao'])\n",
    "\n",
    "num_imagens_treino, num_imagens_validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f79d23-929e-414e-9aed-230617dc9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os DataLoaders para treino e validação\n",
    "# O DataLoader organiza os dados de treinamento e validação para o treinamento da rede neural\n",
    "data_loader_treino = DataLoader(data['treino'], batch_size=tamanho_do_batch, shuffle=True)\n",
    "data_loader_validacao = DataLoader(data['validacao'], batch_size=tamanho_do_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e10d35ac-1976-4a27-87c1-1ab4daa35d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checando algumas imagens para verificar se está tudo correto até aqui:\n",
    "from IPython.display import Image \n",
    "\n",
    "for classe in indice_para_classe.values():\n",
    "    pasta_classe = os.path.join(pasta_treino, classe)\n",
    "    arquivo_imagem = os.listdir(pasta_classe)[1]\n",
    "    img = Image(filename=os.path.join(pasta_classe, arquivo_imagem))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5aaeba-ae0c-4545-92ed-0fa1daa70b28",
   "metadata": {},
   "source": [
    "<b>Transfer Learning:</b>\n",
    "\n",
    "Como temos poucas imagens precisamos de uma rede pré-treinada para nos ajudar na classificação. Vamos usar a Alexnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfcf5f9d-7a77-4935-bd22-62fce743d9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhyan\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rhyan\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\rhyan/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:52<00:00, 4.66MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "384aa24c-de76-422a-809c-9d777ce979d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.models.alexnet.AlexNet"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d175b57-5d3f-4189-97da-abbe038afe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos congelar os parâmetros da rede pré-treinada, para que eles não aprendam tudo de novo:\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "# Isso vai desligar o treinamento e atualização dos coeficientes das camadas da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "163c66cc-c626-4073-9acd-45a39f0b66c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    (7): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precisamos mudar a última camada para alterar o número de classes\n",
    "# O restante da rede foi aproveitado, porém precisamos que ela aprenda os padrões dentro das nossas 4 classes\n",
    "\n",
    "alexnet.classifier[6] = nn.Linear(4096, numero_de_classes)\n",
    "\n",
    "# Incluindo o softmax, que converte efetivamente em probabilidade de ser uma das 4 classas:\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8b8cfb0-6e65-411b-8ad4-194508ab595c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametros_ultima_camada = alexnet.classifier[6].parameters()\n",
    "\n",
    "[parametro for parametro in parametros_ultima_camada][0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c8e5c1c-0845-46c8-a556-a6125ddbfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos utilizar a função de erro de entropia cruzada, o mais utilizado em problemas de classficiação:\n",
    "\n",
    "funcao_erro = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea4a03-70bb-4617-aefa-2638fa3068ef",
   "metadata": {},
   "source": [
    "<b>Otimizador:</b>\n",
    "\n",
    "O otimizador é quem efetivamente altera os pesos da rede de acordo com algum algoritmo. Nesse caso, vamos utilizar o Adam Optimizer, mas o Stochastic Gradient Descent também funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1784ac7-6ed5-49b9-a193-cdef9826a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passando os parâmetros para o otimizador alterá-los:\n",
    "otimizador = optim.Adam(alexnet.parameters())\n",
    "otimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964143b-3788-4130-8b1c-8e5069bc43c7",
   "metadata": {},
   "source": [
    "<b>Treinar e Validar:</b>\n",
    "\n",
    "O treinamento realiza vários caminhos para frente (forward:previsão -- começa com uma previsão fraca e conforme ele anda pra frente e pra trás ele vai melhorando as previsões), cálculos de erro (distancia entre previsão e valor real) e para traz (backward: aprendizado com o erro). A cade \"época\" todas as imagens do treino são utilizadas para otimizar os parâmetros da rede. Dentro de cada \"época\", utilizamos o treinamento em batch, ao invés de imagem por imagem.\n",
    "\n",
    "Na validação não queremos manter o cálculo de gradientes, pois não vamos fazer backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af73010a-bc1c-4e36-b0d0-e07709898666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_e_validar(modelo, metrica_erro, otimizador, epocas=10):\n",
    "    '''\n",
    "    Função para treinamento e validação\n",
    "    Parâmetros\n",
    "        :param modelo: modelo para treinar e validar\n",
    "        :param metrica_erro: critério de erro para minização\n",
    "        :param otimizador: otimizador para alterar os parâmetros da rede\n",
    "        :param epocas: número de épocas (default=10)\n",
    "  \n",
    "    Retorna\n",
    "        melhor_modelo: modelo treinado com a melhor acurácia na validação\n",
    "        historico: (dicionário): histórico com erro no treinamento, erro na validação e acurácia\n",
    "    '''\n",
    "    \n",
    "    # inicializando historico\n",
    "    # a melhor acuracia de validação\n",
    "    # melhor modelo de acordo com a validação\n",
    "    historico = []\n",
    "    melhor_acuracia = 0.0\n",
    "    melhor_modelo = None\n",
    "    \n",
    "    # A CPU provavelmente vai dar conta do recado!\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(device)\n",
    "    \n",
    "    # Cada época perpassa todas as imagens do treino e calcula erros de treino e validação\n",
    "    # para aprendizado da rede neural\n",
    "    for epoca in range(epocas):\n",
    "        inicio_epoca = time.time()\n",
    "        print(\"\\n\\nÉpoca: {}/{}\".format(epoca+1, epocas))\n",
    "        \n",
    "        # Erro e acurácia de treino nessa época\n",
    "        erro_treino = 0.0\n",
    "        acertos_treino = 0.0\n",
    "        \n",
    "        # Erro e acurácia de validação nessa época\n",
    "        erro_validacao = 0.0\n",
    "        acertos_validacao = 0.0\n",
    "        \n",
    "        # Itera a cada lote de imagem. As entradas são os tensores do lote (batch)\n",
    "        # e o label são as classificações de cada imagem do lote: \n",
    "        # batata, cenoura, limão e tomate (0, 1, 2 e 3)\n",
    "        for i, (imagens_lote, y_reais) in enumerate(data_loader_treino):\n",
    "            print(f\"\\nLote: {i+1}\\n\")\n",
    "            \n",
    "            # joga pra GPU ou CPU, dependendo do seu hardware e pytorch instalado\n",
    "            imagens_lote = imagens_lote.to(device)\n",
    "            y_reais = y_reais.to(device)\n",
    "            \n",
    "            # Limpar os gradientes: zerar os gradientes\n",
    "            otimizador.zero_grad()\n",
    "\n",
    "            # Forward pass - calcular saídas a partir das entradas utilizando o modelo\n",
    "            # Como o lote tem 8 imagens, teremos 8 previsões\n",
    "            previsoes = modelo(imagens_lote)\n",
    "\n",
    "\n",
    "            '''\n",
    "            # Vamos entender melhor o que está se passando!\n",
    "            print(imagens_lote.size()) # temos um tensor com 8 imagens\n",
    "            print(previsoes) # temos tensor com 8 previsoes, cada um com 4 valores de log probabilidade\n",
    "            print(torch.exp(previsoes)) # convertendo para exponencial para termos probabilidades de verdade\n",
    "           \n",
    "            print(torch.max(previsoes.data, 1))\n",
    "           \n",
    "            # Dando uma olhada nos y_reais\n",
    "            print(y_reais)\n",
    "\n",
    "\n",
    "            # Vamos dar uma olhadinha nas imagens do batch!\n",
    "            for indice in range(tamanho_do_batch):\n",
    "                tensor_para_imagem = transforms.ToPILImage()\n",
    "                imagem = tensor_para_imagem(imagens_lote[indice])\n",
    "                display(imagem)\n",
    "\n",
    "            break\n",
    "            '''\n",
    "                \n",
    "            # Calcular erro das saidas que foram preditas no forward pass\n",
    "            # comparando com as classificacoes reais (predito vs real)\n",
    "            # e retorna a média dos erros (são 8 erros, lembra?)\n",
    "            erro = metrica_erro(previsoes, y_reais)\n",
    "           \n",
    "            # O backpropagation é a junção do erro.backward() + otimizador.step()\n",
    "            # erro.backward() calcula os gradientes, ou seja, qual a direção \n",
    "            # dos coeficientes para reduzir o erro\n",
    "            # otimizador.step() atualiza os coeficientes de acordo com os gradientes\n",
    "            # calculados no passo anterior\n",
    "            \n",
    "            # Realizar o cálculo dos gradientes a partir do erro de predição\n",
    "            # O otimizador vai utilizar esses gradientes para saber\n",
    "            # qual a direção deve atualizar os coeficientes da rede\n",
    "            erro.backward()\n",
    "            \n",
    "            # Atualizar os parâmetros da rede de acordo com os gradientes calculados no backward\n",
    "            otimizador.step()\n",
    "            \n",
    "            # Daqui pra frente são os cálculos de acurácia\n",
    "            # para avaliar a evolução do modelo durante o treinamento\n",
    "            # nas épocas\n",
    "            \n",
    "            # Calcular o erro total para esse lote (batch) e soma o erro no treino\n",
    "            # O erro calculado já é a média dos 8 erros, portanto temos que multiplicar\n",
    "            # pela quantidade de imagens do lote\n",
    "            erro_treino += erro.item() * imagens_lote.size(0)\n",
    "            \n",
    "            # Cálculo da acurácia\n",
    "            # Pra calcular a acurácia temos que buscar as classes preditas para cada imagem no lote\n",
    "            \n",
    "            # Cada tensor é uma imagem do lote com três posições: probabilidades de cada classe\n",
    "            # torch.max vai retornar o valor da maior probabilidade, \n",
    "            # bem como a posição no tensor (tupla de saída)\n",
    "            # A posição, portanto, vai indicar a classe com maior probabilidade (0,1,2 e 3)\n",
    "            # 1o busca o valor máximo de cada tensor de cada imagem, que vai dar \n",
    "            # a classe final predita\n",
    "            _, classes_previstas = torch.max(previsoes.data, 1)\n",
    "            predicoes_corretas = (classes_previstas == y_reais).type(torch.FloatTensor)\n",
    "            \n",
    "            # Converter os acertos para float e calcular a acurácia média\n",
    "            # do lote\n",
    "            acertos = torch.mean(predicoes_corretas)\n",
    "            \n",
    "            # Calcular a acurácia total de treino do lote todo e adicionar a em train_acc\n",
    "            acertos_treino += torch.sum(predicoes_corretas)\n",
    "            \n",
    "            #print(\"Treino - Lote número {:03d}, Erro: {:.4f}, Acurácia: {:.4f}\".format(i, erro.item(), acuracia.item()))\n",
    "\n",
    "            \n",
    "        # Validação - não é necessário rastrear os gradientes, pois o modelo não vai ser treinado com a validação\n",
    "        # \"desliga\" o autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Mudando de treino para validação\n",
    "            modelo.eval()\n",
    "\n",
    "            # Iteração de validação\n",
    "            for j, (imagens_lote, y_reais) in enumerate(data_loader_validacao):\n",
    "                imagens_lote = imagens_lote.to(device)\n",
    "                y_reais = y_reais.to(device)\n",
    "\n",
    "                # Forward pass de validação\n",
    "                # Previsão do modelo treinado nessa época\n",
    "                previsoes = modelo(imagens_lote)\n",
    "\n",
    "                # Calcular erro de validação\n",
    "                # Previsto versus os verdadeiros hortifruti\n",
    "                erro = metrica_erro(previsoes, y_reais)\n",
    "\n",
    "                # Calcular erro de validação e adicionar a valid_loss\n",
    "                erro_validacao += erro.item() * imagens_lote.size(0)\n",
    "\n",
    "                # Calcular a acurácia de validação\n",
    "                _, classes_previstas = torch.max(previsoes.data, 1)\n",
    "                predicoes_corretas = (classes_previstas == y_reais).type(torch.FloatTensor)\n",
    "\n",
    "                # Converter os acertos para float e calcular a acurácia média\n",
    "                acertos = torch.mean(predicoes_corretas)\n",
    "\n",
    "                # Calcular a acurácia total de validação do lote todo e adicionar a em train_acc\n",
    "                acertos_validacao += torch.sum(predicoes_corretas)\n",
    "\n",
    "                #print(\"Validação - Lote número: {:03d}, Erro: {:.4f}, Acurácia: {:.4f}\".format(j, erro.item(), acuracia.item()))\n",
    "        \n",
    "        #break\n",
    "        \n",
    "        # Calcular a média de erro e acurácia no treino\n",
    "        erro_medio_treino = erro_treino/num_imagens_treino\n",
    "        acuracia_media_treino = acertos_treino/num_imagens_treino\n",
    "\n",
    "        # Calcular a média de erro e acurácia na validação\n",
    "        erro_medio_validacao = erro_validacao/num_imagens_validacao\n",
    "        acuracia_media_validacao = acertos_validacao/num_imagens_validacao\n",
    "\n",
    "        # Incluir no histórico os erros e acurácias méidas\n",
    "        historico.append([erro_medio_treino, erro_medio_validacao, acuracia_media_treino, acuracia_media_validacao])\n",
    "                \n",
    "        fim_epoca = time.time()\n",
    "    \n",
    "        print(\"Época : {:03d}, Treino: Erro: {:.4f}, Acurácia: {:.4f}%, \\n\\t\\tValidação : Erro : {:.4f}, Acurácia: {:.4f}%, Tempo: {:.4f}s\".format(epoca+1, erro_medio_treino, acuracia_media_treino*100, erro_medio_validacao, acuracia_media_validacao*100, fim_epoca-inicio_epoca))\n",
    "        \n",
    "        # Testa se a acurácia na validação desse modelo nessa época é a melhor\n",
    "        # Se for a melhor, salva no melhor modelo e na melhor acurácia\n",
    "        if acuracia_media_validacao > melhor_acuracia:\n",
    "            melhor_acuracia = acuracia_media_validacao\n",
    "            #torch.save(modelo, './modelos/melhor_modelo.pt')\n",
    "            melhor_modelo = modelo\n",
    "\n",
    "    return melhor_modelo, historico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb80576-9f5d-4e66-ad8f-d94fc4a5f196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc4c24-4cad-4dc5-a485-2520754e7584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03cfca-8afa-4012-9bb6-91bce6dbc044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
